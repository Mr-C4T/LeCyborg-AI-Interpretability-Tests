# LeCyborg-AI-Interpretability

This is a **fork of [physical-AI-interpretability](https://github.com/villekuosmanen/physical-AI-interpretability)**, adapted to provide **interpretability tools for the [LeCyborg project](https://github.com/Mr-C4T/LeCyborg)**.

While the original work focused on **visual attention mapping** + **proprioceptive features**, this fork's goal is to extend the attention visualization tools to support our **EMG sensor data** collected from the LeCyborg wearable myo sensor. (ğŸš§ Work in progress ğŸš§)

## ğŸ¦¾ LeCyborg Dataset

<img src="assets/emg_dataset.gif" width="400">

## Visual Attention Mapping

<img src="assets/emg_attention.gif" width="400">

## EMG Attention
<img src="assets/emg_attention_animation_dark.gif" width="400">

## ğŸ“– Original Work & Credits

#### Huge thanks to [villekuosmanen](https://github.com/villekuosmanen) for sharing such great work. 
I encourage anyone interested in interpretability and physical AI to go check his repo and give it a star !

â­ [physical-AI-interpretability](https://github.com/villekuosmanen/physical-AI-interpretability)

You can find the original README here:

ğŸ“ƒ [`old_README.md`](./old_README.md)

