# LeCyborg-AI-Interpretability

This is a **fork of [physical-AI-interpretability](https://github.com/villekuosmanen/physical-AI-interpretability)**, adapted to provide **interpretability tools for the [LeCyborg project](https://github.com/Mr-C4T/LeCyborg)**.

While the original work focused on **visual attention mapping** + **proprioceptive features**, this fork's goal is to extend the attention visualization tools to support our **EMG sensor data** collected from the LeCyborg wearable myo sensor. (ğŸš§ Work in progress ğŸš§)

## ğŸ§  LeCyborg Attention Mapping

<img src="assets/emg_attention.gif" width="480">
<img src="assets/emg_dataset.gif" width="480">

## ğŸ“– Original Work & Credits

#### Huge thanks to [villekuosmanen](https://github.com/villekuosmanen) for the awesome repo and code and method. You can find the original README here:
ğŸ“ƒ [`old_README.md`](./old_README.md)

#### I encourage anyone interested in interpretability and physical AI to go check his work and give it a star !
â­ [physical-AI-interpretability](https://github.com/villekuosmanen/physical-AI-interpretability)
