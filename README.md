# LeCyborg-AI-Interpretability

This is a **fork of [physical-AI-interpretability](https://github.com/villekuosmanen/physical-AI-interpretability)**, adapted to provide **interpretability tools for the [LeCyborg project](https://github.com/Mr-C4T/LeCyborg)**.

While the original work focused on **visual attention mapping** + **proprioceptive features**, this fork's goal is to extend the attention visualization tools to support our **EMG sensor data** collected from the LeCyborg wearable myo sensor. 

## ðŸ§  LeCyborg Attention Mapping

<img src="assets/emg_attention.gif" width="480">
<img src="assets/emg_dataset.gif" width="480">

## ðŸ“– Original Work & Credits

Huge thanks to [villekuosmanen](https://github.com/villekuosmanen) for the original repository and method. You can find the original untouched README here:

ðŸ‘‰ [`old_README.md`](./old_README.md)

I encourage anyone interested in interpretability in physical AI to explore the original work!
